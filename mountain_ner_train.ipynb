{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Now let's fit all of this into a Bert model"
      ],
      "metadata": {
        "id": "T_BdkRUnwdle"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdpFIQ7iAspN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments, AutoTokenizer\n",
        "from torch.utils.data import Dataset, random_split\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import ast\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting the tokens and the sentences"
      ],
      "metadata": {
        "id": "sdBVfptFwp01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/filtered_sentences_tags.csv\")\n",
        "print(df['Tokens'].head())\n",
        "token_lists = df['Tokens'].apply(ast.literal_eval).tolist()\n",
        "tags = df['Tagged Sentence'].apply(ast.literal_eval).tolist()\n",
        "\n",
        "print(token_lists[:5])\n",
        "print(tags[:5])"
      ],
      "metadata": {
        "id": "ngoKZnKO0Z_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions to prepare the dataset for our model, to get train/val/test sets and to tokenize the sentences specifically for BERT. Here we also change annotation to int for the model"
      ],
      "metadata": {
        "id": "ik6travBw56U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(dataset, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
        "    assert train_ratio + val_ratio + test_ratio == 1.0, \"Ratios must sum to 1\"\n",
        "\n",
        "    total_size = len(dataset)\n",
        "    train_size = int(total_size * train_ratio)\n",
        "    val_size = int(total_size * val_ratio)\n",
        "    test_size = total_size - train_size - val_size\n",
        "\n",
        "    return random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "#Loading pre-trained tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "tag2id = {'O': 0, 'B-MOUNTAIN': 1, 'I-MOUNTAIN': 2}\n",
        "id2tag = {id: tag for tag, id in tag2id.items()}\n",
        "\n",
        "#Tokenizing and align labels\n",
        "def tokenize_and_align_labels(token_lists, tags):\n",
        "    tokenized_inputs = tokenizer(token_lists, truncation=True, is_split_into_words=True)\n",
        "    labels = []\n",
        "    for i, label in enumerate(tags):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(tag2id[label[word_idx]])\n",
        "            else:\n",
        "                label_ids.append(tag2id[label[word_idx]] if label[word_idx] == 'I-MOUNTAIN' else -100)\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "# Create class for dataset\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, tokenized_inputs):\n",
        "        self.tokenized_inputs = tokenized_inputs\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.tokenized_inputs.items()}\n",
        "    def __len__(self):\n",
        "        return len(self.tokenized_inputs.input_ids)"
      ],
      "metadata": {
        "id": "BsDCgRePw3qV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aaand the dataset, we save the test set for future inference"
      ],
      "metadata": {
        "id": "l2Xbd02myGVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and create dataset\n",
        "tokenized_inputs = tokenize_and_align_labels(token_lists, tags)\n",
        "full_dataset = NERDataset(tokenized_inputs)\n",
        "train_dataset, val_dataset, test_dataset = split_dataset(full_dataset)\n",
        "test_dataset.to_csv(\"/content/test_data.csv\")"
      ],
      "metadata": {
        "id": "SVxHUBMZyEh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A function for evaluating model performance, regular precision, recall and accuracy"
      ],
      "metadata": {
        "id": "s-ab3UDczAEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "\n",
        "    # Remove ignored index (special tokens)\n",
        "    true_labels = [[l for l in label if l != -100] for label in labels]\n",
        "    true_predictions = [\n",
        "        [p for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(preds, labels)\n",
        "    ]\n",
        "\n",
        "    all_true_labels = sum(true_labels, [])\n",
        "    all_true_predictions = sum(true_predictions, [])\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(all_true_labels, all_true_predictions, average='weighted')\n",
        "    acc = accuracy_score(all_true_labels, all_true_predictions)\n",
        "\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n"
      ],
      "metadata": {
        "id": "8Ucljwvby3i5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model definition, decided to go with 3 epochs and default everything else, turns out it is more than enough"
      ],
      "metadata": {
        "id": "nuXGMPTBysxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Had to create this function for padding, there's probably a better way\n",
        "def custom_collate(batch):\n",
        "    batch_dict = {}\n",
        "    for key in batch[0].keys():\n",
        "        if key == 'labels':\n",
        "            batch_dict[key] = pad_sequence([item[key] for item in batch], batch_first=True, padding_value=-100)\n",
        "        else:\n",
        "            batch_dict[key] = pad_sequence([item[key] for item in batch], batch_first=True, padding_value=0)\n",
        "\n",
        "    return batch_dict\n",
        "\n",
        "model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=len(tag2id))\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=64,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    save_steps=1000,\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=custom_collate,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained(\"./ner_model\")\n",
        "tokenizer.save_pretrained(\"./ner_model\")\n",
        "print(\"Model training completed and saved.\")\n",
        "\n",
        "test_results = trainer.evaluate(test_dataset)\n",
        "print(\"Test set results:\", test_results)\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained(\"./ner_model\")\n",
        "tokenizer.save_pretrained(\"./ner_model\")\n",
        "print(\"Model training completed and saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "F4cRKR26CmVh",
        "outputId": "31af3b68-98ac-486f-f90b-d9f0575c12af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='327' max='327' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [327/327 54:53, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.515300</td>\n",
              "      <td>0.443215</td>\n",
              "      <td>0.878100</td>\n",
              "      <td>0.821106</td>\n",
              "      <td>0.771060</td>\n",
              "      <td>0.878100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.206300</td>\n",
              "      <td>0.162381</td>\n",
              "      <td>0.944725</td>\n",
              "      <td>0.943493</td>\n",
              "      <td>0.942901</td>\n",
              "      <td>0.944725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.115600</td>\n",
              "      <td>0.103574</td>\n",
              "      <td>0.957310</td>\n",
              "      <td>0.959240</td>\n",
              "      <td>0.963120</td>\n",
              "      <td>0.957310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.097500</td>\n",
              "      <td>0.088467</td>\n",
              "      <td>0.961382</td>\n",
              "      <td>0.963430</td>\n",
              "      <td>0.967820</td>\n",
              "      <td>0.961382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.042600</td>\n",
              "      <td>0.077351</td>\n",
              "      <td>0.971252</td>\n",
              "      <td>0.971343</td>\n",
              "      <td>0.971568</td>\n",
              "      <td>0.971252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.061500</td>\n",
              "      <td>0.067729</td>\n",
              "      <td>0.973720</td>\n",
              "      <td>0.974134</td>\n",
              "      <td>0.974792</td>\n",
              "      <td>0.973720</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model training completed and saved.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6/6 01:02]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set results: {'eval_loss': 0.06276286393404007, 'eval_accuracy': 0.978204010462075, 'eval_f1': 0.9786146294465117, 'eval_precision': 0.9794073285621135, 'eval_recall': 0.978204010462075, 'eval_runtime': 72.6178, 'eval_samples_per_second': 5.15, 'eval_steps_per_second': 0.083, 'epoch': 3.0}\n",
            "Model training completed and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training process went pretty neatly, has good metrics on train and validation sets"
      ],
      "metadata": {
        "id": "7iwN4w5t0mjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_ner(sentence):\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    outputs = model(**inputs)\n",
        "    predictions = torch.argmax(outputs.logits, dim=2)\n",
        "\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "    ner_results = []\n",
        "    for token, prediction in zip(tokens, predictions[0]):\n",
        "        if prediction != 0:  # 0 is the ID for 'O'\n",
        "            ner_results.append((token, id2tag[prediction.item()]))\n",
        "\n",
        "    return ner_results\n",
        "# Test the model\n",
        "test_sentence = \"I like to walk and run\"\n",
        "results = perform_ner(test_sentence)\n",
        "print(f\"NER results for '{test_sentence}':\")\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfagzkcbND-g",
        "outputId": "92b9de8e-73c9-4423-a1e2-1125de093869"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NER results for 'I like to walk and run':\n",
            "[]\n"
          ]
        }
      ]
    }
  ]
}